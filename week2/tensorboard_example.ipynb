{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is a visualization tool that makes it easier to understand, debug and optimize Tensorflow programs. For example, you can use TensorBoard to visualize the progress of training loss and accuracy, distribution of internal weights of a neural net or images that pass through the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To record the state during a TensorFlow session, TensorBoard uses computational nodes called *summary operation* that are added to the computation graph.\n",
    "\n",
    "There are several kind of summary operations:\n",
    "- scalar\n",
    "- histogram\n",
    "- tensor summary\n",
    "- image\n",
    "- audio\n",
    "\n",
    "In addition to summary operations, TensorBoard is also able to visualize the computation graph itself. Using the simple regression model for MNIST data, we examine how to attach summary operations to your computation graph and visualize them using TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The standard setup and learning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch and one-hot encode MNIST data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting _tmp_mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting _tmp_mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting _tmp_mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting _tmp_mnist_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('_tmp_mnist_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple regression model, just like we did in the Tensorflow tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # None means that a dimension can be of any length\n",
    "\n",
    "# Weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "# Biases\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Model output\n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "# Correct output\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a small change on top of the TF tutorial: `learning_rate` is defined as a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "learning_rate = tf.Variable(0.1)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard uses a logging directory to. Each project should have it's own logging directory, but we should also name the subdirectory for each task we want to investigate separately. In addition, in this tutorial we add a timestamp so we don't mess up data from different runs, but it depends on your task if you want to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "logdir = \"logs/basic-softmax-regression/run-\" + now.strftime(\"%Y-%m-%dT%H-%M-%S\") + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a bunch of TensorBoard summary variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'weights:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.scalar('accuracy', accuracy)\n",
    "tf.summary.scalar('learning_rate', learning_rate)\n",
    "tf.summary.histogram(\"weights\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `merge_all` convenience TensorFlow variable, we combine all our summary variables to a single computation node. This merged node *depends* on all summary variable nodes. We see later why this is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the computational graph with TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a session. All the computational nodes defined above are part of the default graph and when we create a session without explicitly passing a graph to it, session automatically attaches to the default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard visualizes data in log files written during Tensorflow sessions. To write TensorBoard summary variables to the file, we need to define a log writer. If we pass a graph to it, it will also write a graph to the log file, which allows us to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_writer = tf.summary.FileWriter(logdir, session.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now launch TensorBoard and examine the graph. In your  terminal, write:\n",
    "    \n",
    "    cd <root directory of tensorgroup-study repository>\n",
    "    tensorboard --logdir week2/logs\n",
    "   \n",
    "Now open http://0.0.0.0:6006 to see TensorBoard. Go to `Graph` tab to examine the computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a training funtion that lets us train the model for N epochs. This allos us to manually change learning rate during the training.\n",
    "\n",
    "A few things to note about the training function:\n",
    "\n",
    "1) The `epoch` step counter is defined as a global variable, so that it doens't reset each time we call `train` function. The effect of this can be seen later when examine TensorBoard data. \n",
    "\n",
    "2) We pass the `merged` summary operation to the `run` method. If we don't this, summary operations are not calculated. Tensorflow uses a computational graph, and if nothing depends on summary operations, they are not calculated by default. We could pass each individual summary operation to the `run` method, but `merge_all` node makes it more convenient. As it depends on the all summary operations, it is enough to pass it to trigger computation of all summaries.\n",
    "\n",
    "3) Passing `accuracy` to the `run` is not needed from the perspective of Tensorboard. We use it only for printing, but we can compare the difference between occasionally printed values to more dense Tensorboard data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch = 0 # global step counter\n",
    "def train(epochs):\n",
    "  global epoch\n",
    "  for i in range(epochs):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    summary, accuracy_val, _ = session.run([merged, accuracy, train_step],\n",
    "                                         feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    log_writer.add_summary(summary, epoch)\n",
    "    epoch += 1\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {}, accuracy = {}\".format(epoch, accuracy_val))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model for 1000 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, accuracy = 0.8299999833106995\n",
      "Epoch 200, accuracy = 0.8700000047683716\n",
      "Epoch 300, accuracy = 0.8899999856948853\n",
      "Epoch 400, accuracy = 0.9200000166893005\n",
      "Epoch 500, accuracy = 0.8999999761581421\n",
      "Epoch 600, accuracy = 0.8600000143051147\n",
      "Epoch 700, accuracy = 0.9399999976158142\n",
      "Epoch 800, accuracy = 0.9599999785423279\n",
      "Epoch 900, accuracy = 0.9300000071525574\n",
      "Epoch 1000, accuracy = 0.8999999761581421\n"
     ]
    }
   ],
   "source": [
    "train(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to TensorBoard and examine scalar values and histograms. You should see that TensorBoard has learned the model, but the performance between runs wildly varies. Could this be due to the learning rate?\n",
    "\n",
    "Next, we manually decrease the learning rate to see if it helps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1100, accuracy = 0.9200000166893005\n",
      "Epoch 1200, accuracy = 0.9100000262260437\n",
      "Epoch 1300, accuracy = 0.8899999856948853\n",
      "Epoch 1400, accuracy = 0.9200000166893005\n",
      "Epoch 1500, accuracy = 0.9200000166893005\n",
      "Epoch 1600, accuracy = 0.8999999761581421\n",
      "Epoch 1700, accuracy = 0.9700000286102295\n",
      "Epoch 1800, accuracy = 0.8799999952316284\n",
      "Epoch 1900, accuracy = 0.9300000071525574\n",
      "Epoch 2000, accuracy = 0.8799999952316284\n"
     ]
    }
   ],
   "source": [
    "session.run([tf.assign(learning_rate, 0.01)])\n",
    "train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2100, accuracy = 0.9200000166893005\n",
      "Epoch 2200, accuracy = 0.9200000166893005\n",
      "Epoch 2300, accuracy = 0.8999999761581421\n",
      "Epoch 2400, accuracy = 0.8500000238418579\n",
      "Epoch 2500, accuracy = 0.8700000047683716\n",
      "Epoch 2600, accuracy = 0.8899999856948853\n",
      "Epoch 2700, accuracy = 0.9100000262260437\n",
      "Epoch 2800, accuracy = 0.9200000166893005\n",
      "Epoch 2900, accuracy = 0.9399999976158142\n",
      "Epoch 3000, accuracy = 0.9399999976158142\n"
     ]
    }
   ],
   "source": [
    "session.run([tf.assign(learning_rate, 0.001)])\n",
    "train(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
